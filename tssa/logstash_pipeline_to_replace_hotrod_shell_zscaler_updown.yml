# Last changes were done on the 12 November 2020.
input {
  exec {
    command => "/usr/share/logstash/bin/shellzscalerupdown.sh"
    schedule => "*/60 * * * * * UTC"
    codec => multiline {
      pattern => "\n"
      what => "previous"
    }
  }
}

# Filter for site sla

filter {
  grok {
   tag_on_failure => []
   break_on_match => false
   match => [
            "message",  "zscaler_success\{instance=\"(?<instance>[^\"]+)\",job=\"(?<job>[^\"]+)\"\} (?<ok>\d+) (?<epoch_millis>\d+)"
            ]
  }
  grok {
   tag_on_failure => []
   break_on_match => false
   match => [
            "job",  ".*-(?<sitename>[^-]+\-[^\n]+)$",
            "job",  "^(?<job_status>[^-]+)-(?<device_name>.*)-(?<bu_code>.*)$",
            "instance",  "^\w{2}-(?<country_code>[^-]+)-[^:]+(?<interface>\d+):(?<dest_port>\d+)$"
            ]
  }

if "1" in [ok] {
  mutate {
    add_field => { "up_time" => "60"}
    add_field => { "down_time" => "0"}
    add_field => { "status" => "On" }
    add_field => { "old_ok" => "1" }

 }
}
else if "0" in [ok] {
  mutate {
    add_field => { "up_time" => "0"}
    add_field => { "down_time" => "60"}
    add_field => { "status" => "Off" }
    add_field => { "old_ok" => "0" }

  }
 }
if [status] {
  aggregate {
    task_id => "%{job}"
    aggregate_maps_path => "/var/lib/logstash/.aggregate_map_sh_zscaler"
    timeout => 31556952
    inactivity_timeout => 31556952
    code => "
      map['previous_durable_status'] ||= event.get('status')

      event.set('old_status', event.get('status'))
      event.set('previous_status', map['previous_status'])
      if (event.get('status') == 'On')
      map['shell_duration_up1'] ||= 0
      map['shell_duration_up1'] += 1
      event.set('previous_duration', map['shell_duration_up1'])
      else
        map['shell_duration_up1'] = 0
      end
      if (event.get('status') == 'Off')
        map['shell_duration_down1'] ||= 0
        map['shell_duration_down1'] += 1
        event.set('previous_duration', map['shell_duration_down1'])
      else
        map['shell_duration_down1'] = 0
      end
      if (event.get('previous_duration') > 14)
         map['previous_status'] = event.get('status')
         event.set('previous_epoch', map['previous_epcho'])
      end
      if (event.get('previous_status') == 'On')
      map['shell_duration_up'] ||= 0
      map['shell_duration_up'] += 1
      event.set('duration', map['shell_duration_up'])
      else
        map['shell_duration_up'] = 0
      end
      if (event.get('previous_status') == 'Off')
        map['shell_duration_down'] ||= 0
        map['shell_duration_down'] += 1
        event.set('duration', map['shell_duration_down'])
      else
        map['shell_duration_down'] = 0
      end
    "
  }
 }
if ![previous_status]{
  drop {}
}

  mutate {
    add_field => { "[@timestamp_logstash]" => "%{[@timestamp]}" }
  }

date {
  match => [ "epoch_millis", "UNIX_MS" ]
  timezone => "Africa/Johannesburg"
  }
fingerprint {
  source => "message"
  target => "[@metadata][fingerprint]"
  method => "MD5"
  key => "sla"
 }

ruby {
  code => "event.set('[@metadata][tsprefix]', event.get('@timestamp').to_i.to_s(16))"
 }

mutate {
  convert => [
    "ok", "integer",
    "dest_port", "integer",
    "old_ok", "integer",
    "epoch_millis", "integer",
    "interface", "integer"
   ]
 }

mutate {
  remove_field => ["message", "command", "shell_duration_down", "shell_duration_down1", "shell_duration_up1", "shell_duration_up"]
 }
 if ![job]{
  drop {}
  }
}

# Output to  Elasticsearch cluster

output{
#if ("On" in [status] and [snd_since_tmp] == 900) or ("Off" in [status] and [snd_since_tmp_up] == 900) {
#  stdout{codec=>rubydebug}
  elasticsearch {
    index => "logstash-shell-updown-zscaler-%{+YYYY.MM.dd}"
    hosts => ["196.9.88.116:9200", "196.9.88.117:9200", "196.9.88.118:9200"]
    document_id => "%{[@metadata][tsprefix]}%{[@metadata][fingerprint]}"
   }
#    influxdb {                                                                                                                                                                                
#      host => "192.168.26.104"                                                                                                                                                                      
#      port => 8086                                                                                                                                                                              
#      ssl => "false"                                                                                                                                                                            
#      db => "logstashJDPsiteSLA"                                                                                                                                                                          
#      codec => "json"                                                                                                                                                                          
#      send_as_tags => ["host", "status", "dest_port", "instance", "org", "businesshours", "job", "country_code", "bu_code", "device_name","SCW", "total_business_hours", "target", "siteName", "job_status" ]                                                                                                                              
#      use_event_fields_for_data_points => true
#      data_points => {}
#      exclude_fields => ["@timestamp_logstash", "@timestamp", "@version", "type", "tags", "host"]
#  }
# }
}
